{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 创建prompt 模板\n",
    "question = \"Where is the capital of China? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"google/flan-t5-base\"  # 具体可以参考 https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm  , llm_kwargs = {\"temperature\":0, \"max_length\":512})\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "pypdf package not found, please install it with `pip install pypdf`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda\\envs\\rag_hf\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:236\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[1;34m(self, file_path, password, headers, extract_images, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpypdf\u001b[39;00m  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pypdf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m###加载文件\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data//baichuan.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "File \u001b[1;32md:\\anaconda\\envs\\rag_hf\\lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:238\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[1;34m(self, file_path, password, headers, extract_images, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpypdf\u001b[39;00m  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with `pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(file_path, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(\n\u001b[0;32m    243\u001b[0m     password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[0;32m    244\u001b[0m     extract_images\u001b[38;5;241m=\u001b[39mextract_images,\n\u001b[0;32m    245\u001b[0m     extraction_mode\u001b[38;5;241m=\u001b[39mextraction_mode,\n\u001b[0;32m    246\u001b[0m     extraction_kwargs\u001b[38;5;241m=\u001b[39mextraction_kwargs,\n\u001b[0;32m    247\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: pypdf package not found, please install it with `pip install pypdf`"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "###加载文件\n",
    "loader = PyPDFLoader(\"../data//baichuan.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "###文本切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300,chunk_overlap = 50,)\n",
    "\n",
    "docs = text_splitter.split_documents(pages[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HUGGINGFACEHUB_API_TOKEN, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "query = \"How large is the baichuan2 vocabulary size?\"\n",
    "result_simi = db.similarity_search(query , k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = \"\\n\".join([x.page_content for x in result_simi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt = \"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=augmented_prompt, input_variables=[\"source_knowledge\" ,\"query\"])\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm  , llm_kwargs = {\"temperature\":0, \"max_length\":1024})\n",
    "\n",
    "print(llm_chain.run( {\"source_knowledge\":source_knowledge ,\"query\" : query }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_prompt_2 = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augmented_prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, Model\n",
    "model_dir = snapshot_download(\"baichuan-inc/Baichuan2-7B-Chat\", revision='master')\n",
    "model = Model.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"讲解一下“温故而知新”\"})\n",
    "response = model(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = '''Using the contexts below, answer the query.\n",
    "\n",
    "contexts:\n",
    "have taken both these aspects into account. We\n",
    "have expanded the vocabulary size from 64,000\n",
    "in Baichuan 1 to 125,696, aiming to strike a\n",
    "balance between computational efficiency and\n",
    "model performance.\n",
    "Tokenizer V ocab Size Compression Rate ↓\n",
    "LLaMA 2 32,000 1.037\n",
    "Bloom 250,680 0.501\n",
    "improve after training on more than 2.6 trillion\n",
    "tokens. By sharing these intermediary results,\n",
    "we hope to provide the community with greater\n",
    "insight into the training dynamics of Baichuan 2.\n",
    "Understanding these dynamics is key to unraveling\n",
    "the inner working mechanism of large language\n",
    "Baichuan 2: Open Large-scale Language Models\n",
    "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\n",
    "Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\n",
    "Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\n",
    "\n",
    "query: How large is the baichuan2 vocabulary size?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": content})\n",
    "response = model(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
